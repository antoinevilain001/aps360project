{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the specified directory\n",
    "X_train = np.load('ProcessedInputData/X_train.npy')\n",
    "Y_train = np.load('ProcessedInputData/y_train.npy')\n",
    "X_val = np.load('ProcessedInputData/X_val.npy')\n",
    "Y_val = np.load('ProcessedInputData/y_val.npy')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "val_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few elements of Y_train_tensor:\n",
      "torch.Size([3153])\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0])\n",
      "First few elements of X_train_tensor:\n",
      "torch.Size([3153, 5, 90, 1])\n",
      "Val tensors:\n",
      "torch.Size([394])\n",
      "torch.Size([394, 5, 90, 1])\n"
     ]
    }
   ],
   "source": [
    "# Print the first few elements of the tensors\n",
    "print(\"First few elements of Y_train_tensor:\")\n",
    "print(Y_train_tensor.size())\n",
    "print(Y_train_tensor[:10])  # Print first 10 elements\n",
    "\n",
    "print(\"First few elements of X_train_tensor:\")\n",
    "print(X_train_tensor.size())\n",
    "# print(X_train_tensor[:3])  # Print first 3 elements\n",
    "\n",
    "# each input is 5x90x1 matrix\n",
    "# 5 bc there are 5 eeg channels recordings per sample,\n",
    "# 90 bc i did fourier transform and binned it into 90 different bins from 0.5 to 40 Hz (relevant frequencies for EEG)\n",
    "# the 1 was added just to be the dimension of picture style for CNN so its like black and white\n",
    "\n",
    "print(\"Val tensors:\")\n",
    "print(Y_val_tensor.size())\n",
    "print(X_val_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_dataset = TensorDataset(X_train_tensor[:20], Y_train_tensor[:20]) # take just a few elements\n",
    "sanity_loader = DataLoader(sanity_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)  # Initialize biases to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 115.4306, Accuracy: 55.00%\n",
      "Epoch [2/50], Loss: 3.9472, Accuracy: 50.00%\n",
      "Epoch [3/50], Loss: 29.1186, Accuracy: 65.00%\n",
      "Epoch [4/50], Loss: 0.9319, Accuracy: 60.00%\n",
      "Epoch [5/50], Loss: 0.6777, Accuracy: 60.00%\n",
      "Epoch [6/50], Loss: 0.5764, Accuracy: 65.00%\n",
      "Epoch [7/50], Loss: 0.5146, Accuracy: 65.00%\n",
      "Epoch [8/50], Loss: 0.4705, Accuracy: 65.00%\n",
      "Epoch [9/50], Loss: 0.4359, Accuracy: 65.00%\n",
      "Epoch [10/50], Loss: 0.4068, Accuracy: 75.00%\n",
      "Epoch [11/50], Loss: 0.3833, Accuracy: 80.00%\n",
      "Epoch [12/50], Loss: 0.3665, Accuracy: 95.00%\n",
      "Epoch [13/50], Loss: 0.3558, Accuracy: 95.00%\n",
      "Epoch [14/50], Loss: 0.3475, Accuracy: 95.00%\n",
      "Epoch [15/50], Loss: 0.3362, Accuracy: 95.00%\n",
      "Epoch [16/50], Loss: 0.3292, Accuracy: 95.00%\n",
      "Epoch [17/50], Loss: 0.3219, Accuracy: 95.00%\n",
      "Epoch [18/50], Loss: 0.3188, Accuracy: 90.00%\n",
      "Epoch [19/50], Loss: 0.3163, Accuracy: 95.00%\n",
      "Epoch [20/50], Loss: 0.2949, Accuracy: 100.00%\n",
      "Epoch [21/50], Loss: 0.3075, Accuracy: 95.00%\n",
      "Epoch [22/50], Loss: 0.3345, Accuracy: 100.00%\n",
      "Epoch [23/50], Loss: 0.3220, Accuracy: 100.00%\n",
      "Epoch [24/50], Loss: 0.3120, Accuracy: 100.00%\n",
      "Epoch [25/50], Loss: 0.3050, Accuracy: 100.00%\n",
      "Epoch [26/50], Loss: 0.2990, Accuracy: 100.00%\n",
      "Epoch [27/50], Loss: 0.2935, Accuracy: 100.00%\n",
      "Epoch [28/50], Loss: 0.2881, Accuracy: 100.00%\n",
      "Epoch [29/50], Loss: 0.2833, Accuracy: 100.00%\n",
      "Epoch [30/50], Loss: 0.2783, Accuracy: 100.00%\n",
      "Epoch [31/50], Loss: 0.2743, Accuracy: 100.00%\n",
      "Epoch [32/50], Loss: 0.2584, Accuracy: 100.00%\n",
      "Epoch [33/50], Loss: 0.2476, Accuracy: 100.00%\n",
      "Epoch [34/50], Loss: 0.2397, Accuracy: 100.00%\n",
      "Epoch [35/50], Loss: 0.2329, Accuracy: 100.00%\n",
      "Epoch [36/50], Loss: 0.2276, Accuracy: 100.00%\n",
      "Epoch [37/50], Loss: 0.2201, Accuracy: 100.00%\n",
      "Epoch [38/50], Loss: 0.2255, Accuracy: 95.00%\n",
      "Epoch [39/50], Loss: 0.2216, Accuracy: 100.00%\n",
      "Epoch [40/50], Loss: 0.2120, Accuracy: 100.00%\n",
      "Epoch [41/50], Loss: 0.2048, Accuracy: 100.00%\n",
      "Epoch [42/50], Loss: 0.1995, Accuracy: 100.00%\n",
      "Epoch [43/50], Loss: 0.1942, Accuracy: 100.00%\n",
      "Epoch [44/50], Loss: 0.2192, Accuracy: 100.00%\n",
      "Epoch [45/50], Loss: 0.2036, Accuracy: 100.00%\n",
      "Epoch [46/50], Loss: 0.1940, Accuracy: 100.00%\n",
      "Epoch [47/50], Loss: 0.1863, Accuracy: 100.00%\n",
      "Epoch [48/50], Loss: 0.1824, Accuracy: 100.00%\n",
      "Epoch [49/50], Loss: 0.1775, Accuracy: 100.00%\n",
      "Epoch [50/50], Loss: 0.1736, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)  \n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)  \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Compute correct FC input size dynamically\n",
    "        self._to_linear = None\n",
    "        self._compute_linear_input_size()\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 64)  \n",
    "        self.fc2 = nn.Linear(64, 1)  \n",
    "        \n",
    "\n",
    "    def _compute_linear_input_size(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, 5, 90)  # Dummy input\n",
    "            x = self.pool(self.relu(self.conv1(x)))  \n",
    "            x = self.pool(self.relu(self.conv2(x)))  \n",
    "            self._to_linear = x.numel()  # Compute feature size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 5, 90)  \n",
    "        x = self.pool(self.relu(self.conv1(x)))  \n",
    "        x = self.pool(self.relu(self.conv2(x)))  \n",
    "        x = x.view(x.size(0), -1)  # Flatten dynamically\n",
    "        x = self.relu(self.fc1(x))  \n",
    "        x = self.fc2(x)  \n",
    "        return x  # No sigmoid!\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, epochs=10):\n",
    "    torch.manual_seed(1000)\n",
    "    model.train()\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)  # Use Adam\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.view(-1, 1).float()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Convert logits to probabilities and round\n",
    "            predicted = torch.round(torch.sigmoid(outputs))  \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Initialize Model\n",
    "cnn_model = CNN()\n",
    "cnn_model.apply(init_weights)  # Apply Xavier Initialization\n",
    "\n",
    "# Train Model\n",
    "train_model(model=cnn_model, train_loader=sanity_loader, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lie_detector_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
